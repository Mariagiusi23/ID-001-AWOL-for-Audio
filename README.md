# AWOL for Audio — Language-to-Sound Generation via Parametric Synthesis

This project implements an adaptation of the AWOL framework to the audio domain, enabling controllable sound generation from natural language using CLAP embeddings and FM synthesis.

## 🔗 Report
The final report is available in `report/main.pdf`.

## 📁 Structure
- `notebook/`: Jupyter notebook with the full pipeline
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mariagiusi23/ID-001-AWOL-for-Audio/blob/main/notebook/01_baseline_clap_embedding_to_fm_synthesis.ipynb)

- `images/`: generated spectrograms and plots
- `audio/`: synthesized audio examples (optional)
- `report/`: PDF report for submission

## 🧠 Technologies
- CLAP (Contrastive Language-Audio Pretraining)
- MLP regressor (with optional supervised training)
- FM synthesis implemented in PyTorch

## 🎓 Context
Developed as part of 
