{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariagiusi23/ID-001-AWOL-for-Audio/blob/main/notebook/03_AWOL_RealnvpMapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c_AT_3C9SG3"
      },
      "source": [
        "#**üéØ STEP 1 ‚Äî Prompt & CLAP Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIvKB8D69SG3"
      },
      "source": [
        "**üìç 1.1 ‚Äî Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEe3oCMK9SG4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torchaudio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4qFXU1j9SG4"
      },
      "source": [
        "**üìç 1.2 ‚Äî Import libraries and set device**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEBBfdRC9SG4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ClapModel, ClapProcessor\n",
        "from IPython.display import Audio, display\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAExFYVj9SG4"
      },
      "source": [
        "**üìç 1.3 ‚Äî Load CLAP model and processor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Vwhj4f9SG4"
      },
      "outputs": [],
      "source": [
        "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
        "model_clap = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slgm-tVo9SG4"
      },
      "source": [
        "**üìç 1.4 ‚Äî Prompt ‚Üí Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ5504Nj9SG4"
      },
      "outputs": [],
      "source": [
        "prompt = \"a soft metallic ringing\"\n",
        "inputs = processor(text=prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    embedding = model_clap.get_text_features(**inputs)\n",
        "embedding = embedding.squeeze()\n",
        "print(\"Embedding shape:\", embedding.shape)  # Expected: torch.Size([512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWSxEXFP9SG5"
      },
      "source": [
        "**üìç 1.5 ‚Äî Embedding visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa664l5G9SG5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 2))\n",
        "plt.plot(embedding.cpu().numpy())\n",
        "plt.title(f\"CLAP Embedding for: \\\"{prompt}\\\"\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rJLe1NI9SG5"
      },
      "source": [
        "Note: _The initial steps in this notebook (CLAP model loading and embedding) partially overlap with previous notebooks. This is intentional to ensure full standalone execution and to serve as a clean entry point for the RealNVP-based contribution._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFtu9-5u9SG5"
      },
      "source": [
        "#**üéØ STEP 2 ‚Äî RealNVP Model Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2t_2r79SG5"
      },
      "source": [
        "**üìç 2.1 ‚Äî Define Coupling Layer for RealNVP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oWGJDR09SG5"
      },
      "outputs": [],
      "source": [
        "class CouplingLayer(nn.Module):\n",
        "    def __init__(self, dim, mask):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.mask = mask  # Binary vector of shape (dim,)\n",
        "\n",
        "        hidden = 256\n",
        "        self.scale_net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.translate_net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_masked = x * self.mask\n",
        "        s = self.scale_net(x_masked) * (1 - self.mask)\n",
        "        t = self.translate_net(x_masked) * (1 - self.mask)\n",
        "        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
        "        log_det_jacobian = s.sum(dim=1)\n",
        "        return y, log_det_jacobian\n",
        "\n",
        "    def inverse(self, y):\n",
        "        y_masked = y * self.mask\n",
        "        s = self.scale_net(y_masked) * (1 - self.mask)\n",
        "        t = self.translate_net(y_masked) * (1 - self.mask)\n",
        "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqswYMN09SG5"
      },
      "source": [
        "**üìç 2.2 ‚Äî Build RealNVP Model with Projection to FM Space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDi62lIM9SG5"
      },
      "outputs": [],
      "source": [
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_layers):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            mask = self._get_mask(i)\n",
        "            self.layers.append(CouplingLayer(input_dim, mask))\n",
        "\n",
        "        self.projection = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def _get_mask(self, layer_index):\n",
        "        mask = torch.zeros(self.input_dim)\n",
        "        if layer_index % 2 == 0:\n",
        "            mask[: self.input_dim // 2] = 1\n",
        "        else:\n",
        "            mask[self.input_dim // 2 :] = 1\n",
        "        return mask.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        log_det = 0\n",
        "        for layer in self.layers:\n",
        "            x, ldj = layer(x)\n",
        "            log_det += ldj\n",
        "        return x, log_det\n",
        "\n",
        "    def inverse(self, z):\n",
        "        for layer in reversed(self.layers):\n",
        "            z = layer.inverse(z)\n",
        "        return self.projection(z)\n",
        "\n",
        "# Initialize the model\n",
        "realnvp = RealNVP(input_dim=512, output_dim=4, n_layers=6).to(device)\n",
        "print(\"‚úÖ RealNVP model initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVtycKqm9SG5"
      },
      "source": [
        "#**üéØ STEP 3 ‚Äî Supervised Training: Embedding ‚Üí FM Parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OtqdPTY9SG6"
      },
      "source": [
        "**üìç 3.1 ‚Äî Define training data (same manual prompts as previous notebook)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akitRTA99SG6"
      },
      "outputs": [],
      "source": [
        "train_data = [\n",
        "    {\"prompt\": \"a high whistle\",         \"params\": [0.888, 0.889, 0.95, 0.67]},\n",
        "    {\"prompt\": \"a deep rumbling drone\",  \"params\": [0.022, 0.044, 0.60, 0.56]},\n",
        "    {\"prompt\": \"a mellow flute tone\",    \"params\": [0.378, 0.133, 0.35, 0.44]},\n",
        "    {\"prompt\": \"a robotic beep\",         \"params\": [0.555, 0.333, 0.80, 0.56]},\n",
        "    {\"prompt\": \"a soft crackling fire\",  \"params\": [0.167, 0.022, 0.75, 0.33]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QwCqcNh9SG6"
      },
      "source": [
        "**üìç 3.2 ‚Äî Compute embeddings and build dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qAQ9D559SG6"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "targets = []\n",
        "\n",
        "for example in train_data:\n",
        "    inputs = processor(text=example[\"prompt\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = model_clap.get_text_features(**inputs).squeeze()\n",
        "    embeddings.append(emb)\n",
        "    targets.append(torch.tensor(example[\"params\"]))\n",
        "\n",
        "X = torch.stack(embeddings).to(device)\n",
        "y = torch.stack(targets).to(device)\n",
        "\n",
        "# üìç 3.3 ‚Äî Training loop (MSE on projected inverse)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(realnvp.parameters(), lr=1e-3)\n",
        "n_epochs = 300\n",
        "\n",
        "realnvp.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    z, _ = realnvp(X)              # forward pass\n",
        "    y_pred = realnvp.inverse(z)    # project to FM param space\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} ‚Äî Loss: {loss.item():.6f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Falzt6b9SG6"
      },
      "source": [
        "#**üéØ STEP 4 ‚Äî Evaluation: Audio Generation and Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xzqzv99SG6"
      },
      "source": [
        "**üìç 4.1 ‚Äî Define synthesis function and audio generation utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36Ssc7xf9SG6"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import spectrogram\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import Audio, display\n",
        "import math\n",
        "\n",
        "\n",
        "def synthesize(p, sr=16000, duration=2.0):\n",
        "    t = torch.linspace(0, duration, int(sr * duration))\n",
        "    carrier, modulator, index, amplitude = p\n",
        "    mod_signal = torch.sin(2 * math.pi * modulator * t)\n",
        "    signal = amplitude * torch.sin(2 * math.pi * carrier * t + index * mod_signal)\n",
        "    return signal\n",
        "\n",
        "\n",
        "def generate_audio(prompt, model, processor, clap_model, sr=16000, duration=2.0):\n",
        "    inputs = processor(text=prompt, return_tensors=\"pt\").to(clap_model.device)\n",
        "    with torch.no_grad():\n",
        "        embedding = clap_model.get_text_features(**inputs)\n",
        "        z, _ = model(embedding)\n",
        "        raw_p = model.inverse(z).squeeze()\n",
        "\n",
        "    # Rescale to real-world FM ranges\n",
        "    carrier    = 100 + 900 * raw_p[0]\n",
        "    modulator  = 50 + 450 * raw_p[1]\n",
        "    index      = 10 * raw_p[2]\n",
        "    amplitude  = 0.9 * raw_p[3]\n",
        "    p = [carrier.item(), modulator.item(), index.item(), amplitude.item()]\n",
        "\n",
        "    signal = synthesize(p, sr=sr, duration=duration)\n",
        "    signal_np = signal.numpy()\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"  Carrier freq:   {p[0]:.2f} Hz\")\n",
        "    print(f\"  Modulator freq: {p[1]:.2f} Hz\")\n",
        "    print(f\"  Modulation idx: {p[2]:.2f}\")\n",
        "    print(f\"  Amplitude:      {p[3]:.2f}\")\n",
        "\n",
        "    # Plot only the first 0.1s of the waveform for clarity\n",
        "    plot_duration = 0.1\n",
        "    plot_samples = int(sr * plot_duration)\n",
        "    time_axis = np.linspace(0, plot_duration, plot_samples)\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.plot(time_axis, signal_np[:plot_samples], linewidth=0.3, color='black')\n",
        "    plt.title(\"Waveform (first 0.1s)\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    f, t_spec, Sxx = spectrogram(signal_np, fs=sr, nperseg=512)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(10 * np.log10(Sxx + 1e-8), aspect='auto', origin='lower',\n",
        "               extent=[t_spec.min(), t_spec.max(), f.min(), f.max()], cmap='viridis')\n",
        "    plt.title(\"Spectrogram (dB)\")\n",
        "    plt.colorbar(label=\"dB\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return Audio(signal_np, rate=sr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-tE9OXk9SG6"
      },
      "source": [
        "**üìç 4.2 ‚Äî Test the model on training prompts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqGOqe7Z9SG6"
      },
      "outputs": [],
      "source": [
        "for example in train_data:\n",
        "    display(generate_audio(example[\"prompt\"], realnvp, processor, model_clap, duration=2.0))\n",
        "\n",
        "# üìç 4.3 ‚Äî Test the model on novel prompts\n",
        "test_prompts = [\n",
        "    \"a sharp bell tone\",\n",
        "    \"a thunderous boom\",\n",
        "    \"a gentle whisper\",\n",
        "    \"an alien pulse\",\n",
        "    \"a mechanical grinding\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    display(generate_audio(prompt, realnvp, processor, model_clap, duration=2.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idTlIczN9SG6"
      },
      "source": [
        "#**üéØ Final Remarks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRHBsQxV9SG6"
      },
      "source": [
        "This notebook presents a RealNVP-based supervised mapping from CLAP embeddings to FM synthesis parameters.  \n",
        "After training on a small manually annotated dataset of 5 prompt‚Äìparameter pairs, the invertible model learns to accurately reconstruct all training instances, achieving near-zero loss.\n",
        "\n",
        "Evaluation on unseen prompts demonstrates good generalization:  \n",
        "the model produces semantically plausible FM parameters for new descriptions such as **\"a sharp bell tone\"** and **\"an alien pulse\"**.\n",
        "\n",
        "Compared to the MLP baseline, RealNVP introduces a more expressive and structured transformation.  \n",
        "This opens the door to future extensions, such as latent-space interpolation and unsupervised regularization.\n",
        "\n",
        "The RealNVP model proves to be a viable and interpretable alternative for language-to-sound parameter mapping."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}