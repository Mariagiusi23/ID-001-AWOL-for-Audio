{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**üéØ STEP 1 ‚Äî Prompt Embeddings**"
      ],
      "metadata": {
        "id": "yFufGNB9xOjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 1.1 ‚Äî Install required libraries**"
      ],
      "metadata": {
        "id": "O95460SaxYfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k98nhAkvxNwm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torchaudio plotly scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 1.2 ‚Äî Import libraries and set up device**"
      ],
      "metadata": {
        "id": "dnPwtPL2xjVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ClapProcessor, ClapModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9cFoWVQzxlkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 1.3 ‚Äî Load CLAP model and processor**"
      ],
      "metadata": {
        "id": "_n9WfrSExotB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
        "model_clap = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)"
      ],
      "metadata": {
        "id": "P0MVamDBxtJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 1.4 ‚Äî Define prompts and semantic labels**"
      ],
      "metadata": {
        "id": "rZ1hI9YixwAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data = [\n",
        "    {\"prompt\": \"a soft bell\", \"label\": \"soft\"},\n",
        "    {\"prompt\": \"a mellow flute tone\", \"label\": \"soft\"},\n",
        "    {\"prompt\": \"a gentle whisper\", \"label\": \"soft\"},\n",
        "    {\"prompt\": \"a harsh buzzer\", \"label\": \"harsh\"},\n",
        "    {\"prompt\": \"a robotic beep\", \"label\": \"harsh\"},\n",
        "    {\"prompt\": \"a metallic grinding\", \"label\": \"harsh\"},\n",
        "    {\"prompt\": \"a digital chime\", \"label\": \"digital\"},\n",
        "    {\"prompt\": \"a pulsing tone\", \"label\": \"digital\"},\n",
        "    {\"prompt\": \"an alien pulse\", \"label\": \"digital\"},\n",
        "]"
      ],
      "metadata": {
        "id": "U9Y53SqGxzcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 1.5 ‚Äî Extract CLAP embeddings**"
      ],
      "metadata": {
        "id": "FlAN0qLox2qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "prompts = []\n",
        "\n",
        "for example in prompt_data:\n",
        "    inputs = processor(text=example[\"prompt\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = model_clap.get_text_features(**inputs).squeeze()\n",
        "    embeddings.append(emb.cpu().numpy())\n",
        "    labels.append(example[\"label\"])\n",
        "    prompts.append(example[\"prompt\"])\n",
        "\n",
        "X = np.vstack(embeddings)\n",
        "print(\"‚úÖ Extracted embeddings shape:\", X.shape)"
      ],
      "metadata": {
        "id": "_afxXX6Yx7uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üéØ STEP 2 ‚Äî Interpolation Experiments**"
      ],
      "metadata": {
        "id": "kKDGkgrOyf_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.1 ‚Äî Select two target prompts**"
      ],
      "metadata": {
        "id": "ImJCzn1zylPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select two semantically distant prompts for interpolation\n",
        "prompt_a = \"a soft bell\"\n",
        "prompt_b = \"a harsh buzzer\"\n",
        "\n",
        "idx_a = prompts.index(prompt_a)\n",
        "idx_b = prompts.index(prompt_b)\n",
        "\n",
        "z_a = torch.tensor(X[idx_a])\n",
        "z_b = torch.tensor(X[idx_b])"
      ],
      "metadata": {
        "id": "oHSwBieRyi9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.2 ‚Äî Define interpolation coefficients**"
      ],
      "metadata": {
        "id": "7p-A2HmxyqX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define alpha values for interpolation from z_a to z_b\n",
        "alphas = np.linspace(0, 1, 7)\n",
        "interpolated_z = [(1 - a) * z_a + a * z_b for a in alphas]"
      ],
      "metadata": {
        "id": "1xgrnB1nyu1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.3 ‚Äî Plot interpolated embedding vectors**"
      ],
      "metadata": {
        "id": "0J1HQJ4JyyXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the vector-wise evolution along the interpolation path\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i, z in enumerate(interpolated_z):\n",
        "    plt.plot(z.numpy(), label=f\"Œ±={alphas[i]:.2f}\", alpha=0.6)\n",
        "plt.title(\"Interpolated CLAP Embeddings\")\n",
        "plt.xlabel(\"Embedding Dimension\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vxiG3MkPy1jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.4 ‚Äî Load trained RealNVP model from GitHub**"
      ],
      "metadata": {
        "id": "KghrDaZlzG8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# üîó Download the pretrained model weights (.pt file) from your GitHub repository\n",
        "github_url = \"https://raw.githubusercontent.com/Mariagiusi23/ID-001-AWOL-for-Audio/main/notebook/realnvp_model.pt\"\n",
        "response = requests.get(github_url)\n",
        "response.raise_for_status()\n",
        "weights_buffer = BytesIO(response.content)\n",
        "\n",
        "# üß† Define the RealNVP architecture\n",
        "class CouplingLayer(nn.Module):\n",
        "    def __init__(self, dim, mask):\n",
        "        super().__init__()\n",
        "        hidden = 256\n",
        "        self.mask = mask\n",
        "        self.scale_net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden), nn.ReLU(), nn.Linear(hidden, dim), nn.Tanh()\n",
        "        )\n",
        "        self.translate_net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden), nn.ReLU(), nn.Linear(hidden, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_masked = x * self.mask\n",
        "        s = self.scale_net(x_masked) * (1 - self.mask)\n",
        "        t = self.translate_net(x_masked) * (1 - self.mask)\n",
        "        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
        "        log_det_jacobian = s.sum(dim=1)\n",
        "        return y, log_det_jacobian\n",
        "\n",
        "    def inverse(self, y):\n",
        "        y_masked = y * self.mask\n",
        "        s = self.scale_net(y_masked) * (1 - self.mask)\n",
        "        t = self.translate_net(y_masked) * (1 - self.mask)\n",
        "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
        "        return x\n",
        "\n",
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layers):\n",
        "            mask = self._get_mask(i, input_dim).to(device)\n",
        "            self.layers.append(CouplingLayer(input_dim, mask))\n",
        "        self.projection = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def _get_mask(self, layer_index, dim):\n",
        "        mask = torch.zeros(dim)\n",
        "        if layer_index % 2 == 0:\n",
        "            mask[: dim // 2] = 1\n",
        "        else:\n",
        "            mask[dim // 2 :] = 1\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        log_det = 0\n",
        "        for layer in self.layers:\n",
        "            x, ldj = layer(x)\n",
        "            log_det += ldj\n",
        "        return x, log_det\n",
        "\n",
        "    def inverse(self, z):\n",
        "        for layer in reversed(self.layers):\n",
        "            z = layer.inverse(z)\n",
        "        return self.projection(z)\n",
        "\n",
        "# üì¶ Instantiate the model and load the pretrained weights\n",
        "realnvp = RealNVP(input_dim=512, output_dim=4, n_layers=6).to(device)\n",
        "realnvp.load_state_dict(torch.load(weights_buffer, map_location=device))\n",
        "realnvp.eval()\n",
        "\n",
        "print(\"‚úÖ RealNVP model loaded from GitHub.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KJV_9iAH5MGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.5 ‚Äî Define decoder for parameter mapping**"
      ],
      "metadata": {
        "id": "QRg-IJZ1zO2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that maps a CLAP embedding to FM parameters using RealNVP\n",
        "def real_decoder(z):\n",
        "    z = z.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        z_transformed, _ = realnvp(z)\n",
        "        pred = realnvp.inverse(z_transformed).squeeze()\n",
        "    return pred.cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "o-zGe_SgzRo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.6 ‚Äî Define FM synthesis function**"
      ],
      "metadata": {
        "id": "rcZtjmbc7Vre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate audio signal from a 4-parameter FM vector\n",
        "def synthesize(p, sr=16000, duration=2.0):\n",
        "    t = torch.linspace(0, duration, int(sr * duration))\n",
        "    carrier, modulator, index, amplitude = p\n",
        "    mod_signal = torch.sin(2 * math.pi * modulator * t)\n",
        "    signal = amplitude * torch.sin(2 * math.pi * carrier * t + index * mod_signal)\n",
        "    return signal"
      ],
      "metadata": {
        "id": "TLDA9a0K7VO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.7 ‚Äî Generate audio from interpolated embeddings**"
      ],
      "metadata": {
        "id": "78faHsHszUnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and synthesize audio signals from interpolated embeddings\n",
        "from IPython.display import Audio\n",
        "import math\n",
        "\n",
        "for i, z in enumerate(interpolated_z):\n",
        "    normed_params = real_decoder(z)\n",
        "    carrier   = 100 + 900 * normed_params[0]\n",
        "    modulator = 50 + 450 * normed_params[1]\n",
        "    index     = 10 * normed_params[2]\n",
        "    amplitude = 0.9 * normed_params[3]\n",
        "    p = [carrier, modulator, index, amplitude]\n",
        "    signal = synthesize(p)\n",
        "    audio = Audio(signal.numpy(), rate=16000)\n",
        "\n",
        "    print(f\"\\nŒ± = {alphas[i]:.2f}\")\n",
        "    print(f\"  Carrier freq:   {carrier:.2f} Hz\")\n",
        "    print(f\"  Modulator freq: {modulator:.2f} Hz\")\n",
        "    print(f\"  Modulation idx: {index:.2f}\")\n",
        "    print(f\"  Amplitude:      {amplitude:.2f}\")\n",
        "\n",
        "    display(audio)"
      ],
      "metadata": {
        "id": "qX-SYn4TzYZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.8 ‚Äî Extrapolation beyond alpha = 1**"
      ],
      "metadata": {
        "id": "AcISn8sk8Ww3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_alphas = [1.2, 1.5, 2.0]\n",
        "extra_z = [(1 - a) * z_a + a * z_b for a in extra_alphas]\n",
        "\n",
        "for i, z in enumerate(extra_z):\n",
        "    normed_params = real_decoder(z)\n",
        "    carrier    = 100 + 900 * normed_params[0]\n",
        "    modulator  = 50 + 450 * normed_params[1]\n",
        "    index      = 10 * normed_params[2]\n",
        "    amplitude  = 0.9 * normed_params[3]\n",
        "    p = [carrier, modulator, index, amplitude]\n",
        "\n",
        "    signal = synthesize(p)\n",
        "    audio = Audio(signal.numpy(), rate=16000)\n",
        "\n",
        "    print(f\"\\nŒ± = {extra_alphas[i]:.2f} (extrapolated)\")\n",
        "    print(f\"  Carrier freq:   {carrier:.2f} Hz\")\n",
        "    print(f\"  Modulator freq: {modulator:.2f} Hz\")\n",
        "    print(f\"  Modulation idx: {index:.2f}\")\n",
        "    print(f\"  Amplitude:      {amplitude:.2f}\")\n",
        "    display(audio)\n"
      ],
      "metadata": {
        "id": "W293HZJG8coD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 2.9 ‚Äî Save interpolated audio as .wav**"
      ],
      "metadata": {
        "id": "U9MFsAu1LtJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import soundfile as sf\n",
        "\n",
        "os.makedirs(\"interpolated_audio\", exist_ok=True)\n",
        "\n",
        "for i, z in enumerate(interpolated_z):\n",
        "    normed_params = real_decoder(z)\n",
        "    carrier    = 100 + 900 * normed_params[0]\n",
        "    modulator  = 50 + 450 * normed_params[1]\n",
        "    index      = 10 * normed_params[2]\n",
        "    amplitude  = 0.9 * normed_params[3]\n",
        "    p = [carrier, modulator, index, amplitude]\n",
        "\n",
        "    signal = synthesize(p)\n",
        "\n",
        "    filename = f\"interpolated_audio/audio_interp_{i}_alpha_{alphas[i]:.2f}.wav\"\n",
        "    sf.write(filename, signal.numpy(), samplerate=16000)\n",
        "    print(f\"‚úÖ Saved: {filename}\")\n"
      ],
      "metadata": {
        "id": "vbh4AMRBLwsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üéØ STEP 3 ‚Äî Latent Space Visualization**"
      ],
      "metadata": {
        "id": "ArVq-q_10B-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 3.1 ‚Äî PCA projection to 2D**"
      ],
      "metadata": {
        "id": "OpG4R8hN0HiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for label in set(labels):\n",
        "    idxs = [i for i, l in enumerate(labels) if l == label]\n",
        "    plt.scatter(X_pca[idxs, 0], X_pca[idxs, 1], label=label, alpha=0.7)\n",
        "plt.title(\"CLAP Embeddings - PCA\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7_0eEQdq0K0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 3.2 ‚Äî t-SNE projection (2D)**"
      ],
      "metadata": {
        "id": "jwhU1NvY0O2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tsne = TSNE(n_components=2, perplexity=5, random_state=42).fit_transform(X)\n",
        "fig = px.scatter(\n",
        "    x=X_tsne[:, 0], y=X_tsne[:, 1],\n",
        "    color=labels, text=prompts,\n",
        "    title=\"CLAP Embeddings - t-SNE (2D)\",\n",
        "    labels={\"x\": \"t-SNE 1\", \"y\": \"t-SNE 2\"}\n",
        ")\n",
        "fig.update_traces(textposition='top center')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "IqVxCgJl0SIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 3.3 ‚Äî t-SNE projection (3D)**"
      ],
      "metadata": {
        "id": "iQCS6kbi0uDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tsne_3d = TSNE(n_components=3, perplexity=5, random_state=42).fit_transform(X)\n",
        "fig3d = px.scatter_3d(\n",
        "    x=X_tsne_3d[:, 0],\n",
        "    y=X_tsne_3d[:, 1],\n",
        "    z=X_tsne_3d[:, 2],\n",
        "    color=labels,\n",
        "    text=prompts,\n",
        "    title=\"CLAP Embeddings - t-SNE (3D)\",\n",
        "    labels={\"x\": \"t-SNE 1\", \"y\": \"t-SNE 2\", \"z\": \"t-SNE 3\"}\n",
        ")\n",
        "fig3d.update_traces(marker=dict(size=5), textposition='top center')\n",
        "fig3d.show()"
      ],
      "metadata": {
        "id": "KGZnYEi90wWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üéØ STEP 4 ‚Äî Comparison with Baseline MLP**\n"
      ],
      "metadata": {
        "id": "N7g2j1jAMXvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 4.1 ‚Äî Load MLP baseline model from GitHub**"
      ],
      "metadata": {
        "id": "KvlTJSziMadO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Define the MLP architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=4):  # ‚ö†Ô∏è hidden_dim must match the saved model\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate the model\n",
        "mlp = MLP().to(device)\n",
        "\n",
        "# GitHub raw link to the saved model file\n",
        "mlp_url = \"https://raw.githubusercontent.com/Mariagiusi23/ID-001-AWOL-for-Audio/main/notebook/mlp_baseline_model.pt\"\n",
        "\n",
        "# Download the model weights\n",
        "response = requests.get(mlp_url)\n",
        "response.raise_for_status()\n",
        "buffer = BytesIO(response.content)\n",
        "\n",
        "# Load weights\n",
        "mlp.load_state_dict(torch.load(buffer, map_location=device))\n",
        "mlp.eval()\n",
        "\n",
        "print(\"‚úÖ MLP baseline model loaded from GitHub.\")\n"
      ],
      "metadata": {
        "id": "jMfreNCvMddZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìç 4.2 ‚Äî Generate and save baseline MLP audio**"
      ],
      "metadata": {
        "id": "rxpaA_Qjb2PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"baseline_audio\", exist_ok=True)\n",
        "\n",
        "for i, z in enumerate(interpolated_z):\n",
        "    with torch.no_grad():\n",
        "        pred = mlp(z.to(device)).squeeze()\n",
        "    normed_params = pred.cpu().numpy().tolist()\n",
        "\n",
        "    carrier    = 100 + 900 * normed_params[0]\n",
        "    modulator  = 50 + 450 * normed_params[1]\n",
        "    index      = 10 * normed_params[2]\n",
        "    amplitude  = 0.9 * normed_params[3]\n",
        "    p = [carrier, modulator, index, amplitude]\n",
        "\n",
        "    signal = synthesize(p)\n",
        "    filename = f\"baseline_audio/audio_mlp_alpha_{alphas[i]:.2f}.wav\"\n",
        "    sf.write(filename, signal.numpy(), samplerate=16000)\n",
        "\n",
        "    print(f\"\\nŒ± = {alphas[i]:.2f} ‚Äî MLP\")\n",
        "    print(f\"  Carrier freq:   {carrier:.2f} Hz\")\n",
        "    print(f\"  Modulator freq: {modulator:.2f} Hz\")\n",
        "    print(f\"  Modulation idx: {index:.2f}\")\n",
        "    print(f\"  Amplitude:      {amplitude:.2f}\")\n",
        "    print(f\"‚úÖ Saved: {filename}\")\n",
        "    display(Audio(signal.numpy(), rate=16000))\n"
      ],
      "metadata": {
        "id": "6DOE74bEN2R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üéØ STEP 5 ‚Äî Gradio Demo: Real-time Semantic Interpolation**"
      ],
      "metadata": {
        "id": "e8-O-XJycDfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def generate_interpolated_sound(prompt_a, prompt_b, alpha):\n",
        "    # Encode prompts\n",
        "    inputs_a = processor(text=prompt_a, return_tensors=\"pt\").to(device)\n",
        "    inputs_b = processor(text=prompt_b, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        z_a = model_clap.get_text_features(**inputs_a).squeeze()\n",
        "        z_b = model_clap.get_text_features(**inputs_b).squeeze()\n",
        "\n",
        "    # Interpolate\n",
        "    z = (1 - alpha) * z_a + alpha * z_b\n",
        "\n",
        "    # Predict FM params\n",
        "    normed_params = real_decoder(z)\n",
        "    carrier    = 100 + 900 * normed_params[0]\n",
        "    modulator  = 50 + 450 * normed_params[1]\n",
        "    index      = 10 * normed_params[2]\n",
        "    amplitude  = 0.9 * normed_params[3]\n",
        "    p = [carrier, modulator, index, amplitude]\n",
        "\n",
        "    # Synthesize\n",
        "    signal = synthesize(p)\n",
        "    return (16000, signal.numpy())\n",
        "\n",
        "# UI layout\n",
        "gr.Interface(\n",
        "    fn=generate_interpolated_sound,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt A\", value=\"a soft bell\"),\n",
        "        gr.Textbox(label=\"Prompt B\", value=\"a harsh buzzer\"),\n",
        "        gr.Slider(0, 1, value=0.5, label=\"Œ± ‚Äî Interpolation factor\")\n",
        "    ],\n",
        "    outputs=gr.Audio(label=\"Generated Audio\"),\n",
        "    title=\"AWOL: Language-to-Sound Interpolation\",\n",
        "    description=\"Type two prompts and slide alpha to explore semantic audio interpolation.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "weKE9tZ9OFeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üéØ Final Remarks**"
      ],
      "metadata": {
        "id": "JlFpLdCu1nnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook explores the semantic latent space of CLAP embeddings by performing smooth interpolation between two audio prompts. The decoded parameters are mapped through a pretrained RealNVP model,resulting in frequency-modulated signals synthesized using a parametric FM decoder. The transition is perceptually continuous, both visually (via latent plots) and sonically (via waveform/audio). This confirms the model's ability to learn a meaningful latent space for controllable sound generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "jUKZU8Ko1F2o"
      }
    }
  ]
}