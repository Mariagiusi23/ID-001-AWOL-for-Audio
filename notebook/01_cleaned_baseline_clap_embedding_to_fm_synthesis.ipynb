{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariagiusi23/ID-001-AWOL-for-Audio/blob/main/notebook/01_cleaned_baseline_clap_embedding_to_fm_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju6DCTFDFI5v"
      },
      "source": [
        "#**üéØ STEP 1 ‚Äî Text Embedding with CLAP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az6JlpLhFRdp"
      },
      "source": [
        "**üìç 1.1 ‚Äî Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I016LvK5c84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, ClapModel\n",
        "\n",
        "# Select device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F6Lm1fE5c85"
      },
      "outputs": [],
      "source": [
        "# Load CLAP model and processor from Hugging Face\n",
        "model_clap = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCm2zKjLFxBs"
      },
      "source": [
        "**üìç 1.2 ‚Äî Prompt embedding example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdVhIptq5c85"
      },
      "outputs": [],
      "source": [
        "prompt = \"a soft metallic ringing\"\n",
        "inputs = processor(text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    embedding = model_clap.get_text_features(**inputs)\n",
        "\n",
        "print(\"Embedding shape:\", embedding.shape)\n",
        "print(\"First 5 values:\", embedding[0, :5])\n",
        "\n",
        "# The embedding is a 512-dimensional vector that encodes the semantic meaning of the prompt.\n",
        "# It will be used as input to a neural network that maps it to sound synthesis parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AG17MrwF9nc"
      },
      "source": [
        "#**üéØ STEP 2 ‚Äî From Embedding to Sound via Parametric Synthesis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4-H1ZOxGAsz"
      },
      "source": [
        "**üìç 2.1 ‚Äî Parametric FM Synthesizer (procedural)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F87cKAJt5c85"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def synthesize(p, sr=16000, duration=1.0):\n",
        "    \"\"\"\n",
        "    Generate sound using a simple FM synthesis algorithm.\n",
        "\n",
        "    Args:\n",
        "        p (list): [carrier_freq, modulator_freq, modulation_index, amplitude]\n",
        "        sr (int): sampling rate\n",
        "        duration (float): duration of the sound in seconds\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: waveform signal\n",
        "    \"\"\"\n",
        "    t = torch.linspace(0, duration, int(sr * duration))\n",
        "    carrier, modulator, index, amplitude = p\n",
        "    mod_signal = torch.sin(2 * math.pi * modulator * t)\n",
        "    signal = amplitude * torch.sin(2 * math.pi * carrier * t + index * mod_signal)\n",
        "    return signal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy9_zYWeGLx8"
      },
      "source": [
        "**üìç 2.2 ‚Äî MLP: CLAP Embedding ‚Üí FM Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSLvm6kE5c85"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EmbedToParams(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 4),\n",
        "            nn.Sigmoid()  # normalize outputs to [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model_mlp = EmbedToParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9zfnXKbGY72"
      },
      "source": [
        "**üìç 2.3 ‚Äî Full Pipeline: prompt ‚Üí parameters ‚Üí audio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqlQBXeT5c86"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import spectrogram\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "\n",
        "def generate_audio(prompt, model, processor, clap_model, sr=16000, duration=0.1):\n",
        "    inputs = processor(text=prompt, return_tensors=\"pt\").to(clap_model.device)\n",
        "    with torch.no_grad():\n",
        "        embedding = clap_model.get_text_features(**inputs)\n",
        "        raw_p = model(embedding).squeeze()\n",
        "\n",
        "    carrier    = 100 + 900 * raw_p[0]\n",
        "    modulator  = 50 + 450 * raw_p[1]\n",
        "    index      = 10 * raw_p[2]\n",
        "    amplitude  = 0.9 * raw_p[3]\n",
        "    p = [carrier.item(), modulator.item(), index.item(), amplitude.item()]\n",
        "\n",
        "    signal = synthesize(p, sr=sr, duration=duration)\n",
        "    signal_np = signal.numpy()\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"  Carrier freq:   {p[0]:.2f} Hz\")\n",
        "    print(f\"  Modulator freq: {p[1]:.2f} Hz\")\n",
        "    print(f\"  Modulation idx: {p[2]:.2f}\")\n",
        "    print(f\"  Amplitude:      {p[3]:.2f}\")\n",
        "\n",
        "    time_axis = np.linspace(0, duration, len(signal_np))\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.plot(time_axis, signal_np, linewidth=0.3, color='black')\n",
        "    plt.title(\"Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    f, t_spec, Sxx = spectrogram(signal_np, fs=sr, nperseg=512)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(10 * np.log10(Sxx + 1e-8), aspect='auto', origin='lower',\n",
        "               extent=[t_spec.min(), t_spec.max(), f.min(), f.max()], cmap='viridis')\n",
        "    plt.title(\"Spectrogram (dB)\")\n",
        "    plt.colorbar(label=\"dB\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return Audio(signal_np, rate=sr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRyKgdiBGwAr"
      },
      "source": [
        "**üìç 2.4 ‚Äî Usage examples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImvjS44d5c86"
      },
      "outputs": [],
      "source": [
        "example_prompts = [\n",
        "    \"a deep rumbling drone\",\n",
        "    \"a high-pitched whistle\",\n",
        "    \"a soft crackling fire\",\n",
        "    \"a robotic beep sequence\",\n",
        "    \"a mellow flute tone\"\n",
        "]\n",
        "\n",
        "for prompt in example_prompts:\n",
        "    display(generate_audio(prompt, model_mlp, processor, model_clap))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HsVD1IJL61d"
      },
      "source": [
        "# **üìå Final comment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OokgbBYFLwv5"
      },
      "source": [
        "This notebook demonstrates a complete baseline system for language-to-sound generation.\n",
        "It shows how CLAP embeddings can be mapped to FM synthesis parameters using a simple MLP.\n",
        "However, the generated sounds do not yet reflect semantic differences between prompts ‚Äî this is expected, since the model has not been trained. This baseline will serve as a reference point for evaluating improvements introduced through supervised learning in the next notebook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}